{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We can load any data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  We create a list of candidate keywords from the document\n",
    "\n",
    "### n_gram_range decides if we just need a keywords or a keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "n_gram_range = (4, 4)\n",
    "stop_words = \"english\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract the keywords, keyphrases using CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training data',\n",
       " 'algorithm correctly determine class',\n",
       " 'algorithm generalize training data',\n",
       " 'allow algorithm correctly determine',\n",
       " 'analyzes training data produces',\n",
       " 'based example input output',\n",
       " 'called supervisory signal supervised',\n",
       " 'class labels unseen instances',\n",
       " 'consisting input object typically',\n",
       " 'consisting set training examples',\n",
       " 'correctly determine class labels',\n",
       " 'data consisting set training',\n",
       " 'data produces inferred function',\n",
       " 'data unseen situations reasonable',\n",
       " 'desired output value called',\n",
       " 'determine class labels unseen',\n",
       " 'example input output pairs',\n",
       " 'example pair consisting input',\n",
       " 'examples optimal scenario allow',\n",
       " 'examples supervised learning example',\n",
       " 'function labeled training data',\n",
       " 'function maps input output',\n",
       " 'function used mapping new',\n",
       " 'generalize training data unseen',\n",
       " 'inferred function used mapping',\n",
       " 'infers function labeled training',\n",
       " 'input object typically vector',\n",
       " 'input output based example',\n",
       " 'input output pairs infers',\n",
       " 'instances requires learning algorithm',\n",
       " 'labeled training data consisting',\n",
       " 'labels unseen instances requires',\n",
       " 'learning algorithm analyzes training',\n",
       " 'learning algorithm generalize training',\n",
       " 'learning example pair consisting',\n",
       " 'learning function maps input',\n",
       " 'learning machine learning task',\n",
       " 'learning task learning function',\n",
       " 'machine learning task learning',\n",
       " 'mapping new examples optimal',\n",
       " 'maps input output based',\n",
       " 'new examples optimal scenario',\n",
       " 'object typically vector desired',\n",
       " 'optimal scenario allow algorithm',\n",
       " 'output based example input',\n",
       " 'output pairs infers function',\n",
       " 'output value called supervisory',\n",
       " 'pair consisting input object',\n",
       " 'pairs infers function labeled',\n",
       " 'produces inferred function used',\n",
       " 'reasonable way inductive bias',\n",
       " 'requires learning algorithm generalize',\n",
       " 'scenario allow algorithm correctly',\n",
       " 'set training examples supervised',\n",
       " 'signal supervised learning algorithm',\n",
       " 'situations reasonable way inductive',\n",
       " 'supervised learning algorithm analyzes',\n",
       " 'supervised learning example pair',\n",
       " 'supervised learning machine learning',\n",
       " 'supervisory signal supervised learning',\n",
       " 'task learning function maps',\n",
       " 'training data consisting set',\n",
       " 'training data produces inferred',\n",
       " 'training data unseen situations',\n",
       " 'training examples supervised learning',\n",
       " 'typically vector desired output',\n",
       " 'unseen instances requires learning',\n",
       " 'unseen situations reasonable way',\n",
       " 'used mapping new examples',\n",
       " 'value called supervisory signal',\n",
       " 'vector desired output value']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range = n_gram_range, \n",
    "                        stop_words = stop_words).fit([document])\n",
    "candidates = count.get_feature_names()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy_sentence_bert\n",
    "# nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')\n",
    "# nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "## We convert both the document and the candidate keywords, keyphrases to numerical data using `SentenceTransformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding = model.encode([document])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cosine Similarity\n",
    "### We find candidates that are most similar to the document using `cosine_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training data',\n",
       " 'learning algorithm analyzes training',\n",
       " 'supervised learning algorithm analyzes',\n",
       " 'algorithm generalize training data',\n",
       " 'learning algorithm generalize training']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Diversification\n",
    "### We diversify the keywords, keyphrases so that we do not get similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Max Sum Similarity\n",
    "### We want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    \n",
    "    # Calculate distances and extract keywords\n",
    "    distance = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distance_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distance.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distance_candidates = distance_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distance_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower nr_candidate value results seem to be very similar to our original cosine similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning task learning',\n",
       " 'learning function maps input',\n",
       " 'signal supervised learning algorithm',\n",
       " 'algorithm analyzes training data',\n",
       " 'algorithm generalize training data']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum(doc_embedding, candidate_embeddings, candidates, top_n = 5, nr_candidates = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher nr_candidate value results will create more diverse keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm correctly determine class',\n",
       " 'analyzes training data produces',\n",
       " 'instances requires learning algorithm',\n",
       " 'learning function maps input',\n",
       " 'supervised learning machine learning']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum(doc_embedding, candidate_embeddings, candidates, top_n = 5, nr_candidates = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `We need to keep nr_candidates less than 20% of the total number of unique words in your document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Maximal Marginal Relevance\n",
    "### MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks\n",
    "### We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords and keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "    \n",
    "    # Extract similarity within words\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    \n",
    "    # Extract similarity between words and the document\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "    \n",
    "    # Initialize candidates and choose the best keywords, keyphrases\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "    \n",
    "    for _ in range(top_n - 1):\n",
    "        \n",
    "        # Extract similarities within candidates and between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we set a relatively low diversity, then our results seem to be very similar to our original cosine similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning algorithm generalize training',\n",
       " 'supervised learning algorithm analyzes',\n",
       " 'algorithm analyzes training data',\n",
       " 'algorithm generalize training data',\n",
       " 'supervised learning machine learning']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n = 5, diversity = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A relatively high diversity score will create very diverse keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning algorithm generalize training',\n",
       " 'data unseen situations reasonable',\n",
       " 'new examples optimal scenario',\n",
       " 'value called supervisory signal',\n",
       " 'algorithm correctly determine class']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n = 5, diversity = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
